2025-01-28 21:18:48,089 - INFO - Using device: mps
2025-01-28 21:18:50,796 - INFO - 
Model Architecture:
2025-01-28 21:18:50,796 - INFO - ==================================================
2025-01-28 21:18:50,796 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-28 21:18:50,797 - INFO - ==================================================
2025-01-28 21:18:50,798 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-28 21:18:50,798 - INFO - ==================================================

2025-01-28 21:18:51,148 - INFO - Set pad_token to eos_token
2025-01-28 21:18:51,151 - INFO - Loaded 7222 text segments from input.txt
2025-01-28 21:18:51,151 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-28 23:05:42,805 - INFO - Using device: mps
2025-01-28 23:05:45,359 - INFO - 
Model Architecture:
2025-01-28 23:05:45,359 - INFO - ==================================================
2025-01-28 23:05:45,359 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-28 23:05:45,360 - INFO - ==================================================
2025-01-28 23:05:45,361 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-28 23:05:45,361 - INFO - ==================================================

2025-01-28 23:05:45,817 - INFO - Set pad_token to eos_token
2025-01-28 23:05:45,820 - INFO - Loaded 7222 text segments from input.txt
2025-01-28 23:05:45,820 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-29 10:11:03,769 - INFO - Using device: mps
2025-01-29 10:11:06,580 - INFO - 
Model Architecture:
2025-01-29 10:11:06,581 - INFO - ==================================================
2025-01-29 10:11:06,581 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-29 10:11:06,582 - INFO - ==================================================
2025-01-29 10:11:06,582 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-29 10:11:06,582 - INFO - ==================================================

2025-01-29 10:11:07,184 - INFO - Set pad_token to eos_token
2025-01-29 10:11:07,188 - INFO - Loaded 7222 text segments from input.txt
2025-01-29 10:11:07,188 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-29 10:14:23,735 - INFO - Using device: cpu
2025-01-29 10:14:26,128 - INFO - 
Model Architecture:
2025-01-29 10:14:26,128 - INFO - ==================================================
2025-01-29 10:14:26,128 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-29 10:14:26,129 - INFO - ==================================================
2025-01-29 10:14:26,129 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-29 10:14:26,129 - INFO - ==================================================

2025-01-29 10:14:26,480 - INFO - Set pad_token to eos_token
2025-01-29 10:14:26,483 - INFO - Loaded 7222 text segments from input.txt
2025-01-29 10:14:26,483 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-29 10:15:13,193 - INFO - Using device: cpu
2025-01-29 10:15:15,572 - INFO - 
Model Architecture:
2025-01-29 10:15:15,572 - INFO - ==================================================
2025-01-29 10:15:15,572 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-29 10:15:15,573 - INFO - ==================================================
2025-01-29 10:15:15,574 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-29 10:15:15,574 - INFO - ==================================================

2025-01-29 10:15:15,933 - INFO - Set pad_token to eos_token
2025-01-29 10:15:15,935 - INFO - Loaded 7222 text segments from input.txt
2025-01-29 10:15:15,936 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-29 12:16:06,524 - INFO - Using device: cpu
2025-01-29 12:16:08,850 - INFO - 
Model Architecture:
2025-01-29 12:16:08,851 - INFO - ==================================================
2025-01-29 12:16:08,851 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-29 12:16:08,852 - INFO - ==================================================
2025-01-29 12:16:08,852 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-29 12:16:08,852 - INFO - ==================================================

2025-01-29 12:16:09,310 - INFO - Set pad_token to eos_token
2025-01-29 12:16:09,330 - INFO - Loaded 7222 text segments from input.txt
2025-01-29 12:16:09,330 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-29 15:33:51,647 - INFO - Using device: cpu
2025-01-29 15:33:53,984 - INFO - 
Model Architecture:
2025-01-29 15:33:53,984 - INFO - ==================================================
2025-01-29 15:33:53,984 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-29 15:33:53,986 - INFO - ==================================================
2025-01-29 15:33:53,986 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-29 15:33:53,986 - INFO - ==================================================

2025-01-29 15:33:54,432 - INFO - Set pad_token to eos_token
2025-01-29 15:33:54,455 - INFO - Loaded 7222 text segments from input.txt
2025-01-29 15:33:54,456 - INFO - No checkpoint found at 5000 steps. Starting fresh training.
2025-01-29 17:01:21,508 - INFO - 
Sample generation at step 500:
2025-01-29 17:01:21,512 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-29 17:01:21,512 - INFO - Generated: Once upon a time, in a distant galaxy such<|endoftext|>

2025-01-29 17:01:23,666 - INFO - Checkpoint saved at step 500
2025-01-29 18:17:22,085 - INFO - 
Sample generation at step 1000:
2025-01-29 18:17:22,087 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-29 18:17:22,087 - INFO - Generated: Once upon a time, in a distant galaxyfeit for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for forhostle should should shouldile thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou

2025-01-29 18:17:24,174 - INFO - Checkpoint saved at step 1000
2025-01-29 19:34:02,778 - INFO - 
Sample generation at step 1500:
2025-01-29 19:34:02,779 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-29 19:34:02,779 - INFO - Generated: Once upon a time, in a distant galaxyre heard heard heard heard heard heard for death;;;;;;;;<|endoftext|>

2025-01-29 19:34:04,716 - INFO - Checkpoint saved at step 1500
2025-01-29 20:46:42,668 - INFO - 
Sample generation at step 2000:
2025-01-29 20:46:42,670 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-29 20:46:42,670 - INFO - Generated: Once upon a time, in a distant galaxyanus hope sovereign you<|endoftext|>

2025-01-29 20:46:44,536 - INFO - Checkpoint saved at step 2000
2025-01-29 21:56:37,411 - INFO - 
Sample generation at step 2500:
2025-01-29 21:56:37,413 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-29 21:56:37,413 - INFO - Generated: Once upon a time, in a distant galaxy never never never never<|endoftext|>

2025-01-29 21:56:39,381 - INFO - Checkpoint saved at step 2500
2025-01-29 23:11:03,462 - INFO - 
Sample generation at step 3000:
2025-01-29 23:11:03,464 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-29 23:11:03,464 - INFO - Generated: Once upon a time, in a distant galaxy YUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUTUCUCUTUTUTUTUTUTUT

2025-01-29 23:11:05,436 - INFO - Checkpoint saved at step 3000
2025-01-30 00:26:16,622 - INFO - 
Sample generation at step 3500:
2025-01-30 00:26:16,623 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-30 00:26:16,623 - INFO - Generated: Once upon a time, in a distant galaxy<|endoftext|>

2025-01-30 00:26:18,586 - INFO - Checkpoint saved at step 3500
2025-01-30 01:36:30,577 - INFO - 
Sample generation at step 4000:
2025-01-30 01:36:30,579 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-30 01:36:30,579 - INFO - Generated: Once upon a time, in a distant galaxyiediedLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL not not not not not<|endoftext|>

2025-01-30 01:36:32,495 - INFO - Checkpoint saved at step 4000
2025-01-30 07:21:32,889 - INFO - 
Sample generation at step 4500:
2025-01-30 07:21:32,891 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-30 07:21:32,891 - INFO - Generated: Once upon a time, in a distant galaxyGood had say or<|endoftext|>

2025-01-30 07:21:34,770 - INFO - Checkpoint saved at step 4500
2025-01-30 08:34:57,637 - INFO - 
Sample generation at step 5000:
2025-01-30 08:34:57,639 - INFO - Prompt: Once upon a time, in a distant galaxy
2025-01-30 08:34:57,639 - INFO - Generated: Once upon a time, in a distant galaxy<|endoftext|>

2025-01-30 08:34:59,555 - INFO - Checkpoint saved at step 5000
2025-01-30 08:35:00,965 - INFO - Checkpoint saved at step 5000
2025-01-30 08:35:00,965 - INFO - Training completed!
2025-01-30 08:37:30,402 - INFO - Using device: cpu
2025-01-30 08:37:32,737 - INFO - 
Model Architecture:
2025-01-30 08:37:32,737 - INFO - ==================================================
2025-01-30 08:37:32,737 - INFO - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(49152, 576)
    (layers): ModuleList(
      (0-29): 30 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=576, out_features=576, bias=False)
          (k_proj): Linear(in_features=576, out_features=192, bias=False)
          (v_proj): Linear(in_features=576, out_features=192, bias=False)
          (o_proj): Linear(in_features=576, out_features=576, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
          (up_proj): Linear(in_features=576, out_features=1536, bias=False)
          (down_proj): Linear(in_features=1536, out_features=576, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-01-30 08:37:32,738 - INFO - ==================================================
2025-01-30 08:37:32,738 - INFO - 
Total trainable parameters: 134,515,008 (134.52M)
2025-01-30 08:37:32,739 - INFO - ==================================================

2025-01-30 08:37:36,054 - INFO - Set pad_token to eos_token
2025-01-30 08:37:36,077 - INFO - Loaded 7222 text segments from input.txt
2025-01-30 08:37:36,077 - INFO - Found checkpoint at 5000 steps. Loading and training for 50 more steps.
2025-01-30 08:44:46,443 - INFO - Checkpoint saved at step 5050
2025-01-30 08:44:46,444 - INFO - Training completed!
